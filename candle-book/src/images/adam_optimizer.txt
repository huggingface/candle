Adam Optimizer Visualization Description

This file describes the design for an Adam optimizer visualization to be created and saved as adam_optimizer.png.

OVERALL LAYOUT:
- A comprehensive visualization showing the key components and workflow of the Adam optimization algorithm
- Clean, professional style with consistent colors and clear labels
- Size approximately 800x500 pixels
- Split into three main sections: moment updates (left), bias correction (center), and parameter updates (right)

COMPONENTS:

1. ALGORITHM OVERVIEW (Top):
   - A flowchart showing the main steps of the Adam algorithm:
     * Input gradients → First moment update → Second moment update → Bias correction → Parameter update
   - Use arrows to show the flow between steps
   - Include the iteration counter t to show how it affects the bias correction
   - Color: Light blue background for this section (#D6EAF8 or similar)

2. MOMENT UPDATES (Left side):
   - Visualization of the exponentially weighted moving averages:
     * First moment (m_t): Show how it tracks the mean of gradients
     * Second moment (v_t): Show how it tracks the squared gradients
   - Include the update equations:
     * m_t = β₁·m_{t-1} + (1-β₁)·∇θJ(θ_t)
     * v_t = β₂·v_{t-1} + (1-β₂)·[∇θJ(θ_t)]²
   - Use a graph showing how these moments evolve over iterations
   - Highlight how β₁ and β₂ control the decay rate
   - Color: Light green background (#D5F5E3 or similar)

3. BIAS CORRECTION (Center):
   - Visualization of the bias correction process:
     * Show how m_t and v_t are biased toward zero at the beginning
     * Show the correction factors: 1/(1-β₁ᵗ) and 1/(1-β₂ᵗ)
     * Show the corrected values: m̂_t and v̂_t
   - Include a graph showing how the correction factor changes with t
   - Highlight the importance of bias correction in early iterations
   - Color: Light purple background (#E8DAEF or similar)

4. PARAMETER UPDATES (Right side):
   - Visualization of the parameter update rule:
     * Show how the learning rate is adapted based on the moments
     * Include the update equation: θ_{t+1} = θ_t - α·m̂_t/√(v̂_t+ε)
   - Show a 2D visualization of how parameters move in the parameter space
   - Compare with SGD and RMSProp to highlight Adam's advantages
   - Color: Light orange background (#FAE5D3 or similar)

5. HYPERPARAMETER EFFECTS (Bottom):
   - Small insets showing the effects of different hyperparameters:
     * Learning rate (α): Too small, too large, just right
     * β₁: Effect on momentum (typically 0.9)
     * β₂: Effect on adaptive learning rate (typically 0.999)
     * ε: Effect on numerical stability (typically 1e-8)
   - Use small graphs or illustrations for each hyperparameter
   - Color: Light yellow background (#FEF9E7 or similar)

ANNOTATIONS:
- Add clear labels for all components and equations
- Include the key equations from the Adam algorithm:
  * m_t = β₁·m_{t-1} + (1-β₁)·∇θJ(θ_t)
  * v_t = β₂·v_{t-1} + (1-β₂)·[∇θJ(θ_t)]²
  * m̂_t = m_t/(1-β₁ᵗ)
  * v̂_t = v_t/(1-β₂ᵗ)
  * θ_{t+1} = θ_t - α·m̂_t/√(v̂_t+ε)
- Add a brief title at the top: "Adam Optimizer: Adaptive Moment Estimation"
- Include a small legend explaining the color coding and symbols
- Add brief explanations of key concepts:
  * First moment: Tracks mean of gradients (like momentum)
  * Second moment: Tracks variance of gradients (like RMSProp)
  * Bias correction: Counteracts initialization bias
  * Adaptive learning rates: Different parameters get different effective learning rates

STYLE GUIDELINES:
- Use a clean, minimalist design with adequate white space
- Use a consistent, professional font (e.g., Arial or Helvetica)
- Use a color scheme that's easy to distinguish but not too bright
- Ensure all text is readable at the intended display size
- Use thin lines for connections and thicker lines for highlighted elements
- Use mathematical notation consistent with the chapter text

SPECIFIC DETAILS:
- For the moment updates, show a simple 1D example with:
  * A gradient that oscillates but has a clear direction
  * The first moment (m_t) smoothing out the oscillations
  * The second moment (v_t) adapting to the gradient magnitude
- For the bias correction, show:
  * How m_t and v_t are biased toward zero at t=1,2,3
  * How the correction factors grow from ~1.0 to ~1.1 to ~1.01
  * The corrected values m̂_t and v̂_t
- For the parameter updates, show:
  * A 2D contour plot of a loss function (e.g., Rosenbrock function)
  * Trajectories for SGD, RMSProp, and Adam
  * How Adam combines the benefits of both

RECOMMENDED TOOLS:
- Python with matplotlib and numpy for creating the visualization
- Use matplotlib's plotting capabilities for the graphs and contour plots
- Consider using networkx for the flowchart elements

EXPORT INSTRUCTIONS:
- Export as PNG at 800x500 pixels resolution
- Save as "adam_optimizer.png" in the src/images/ directory
- Ensure the image has a transparent background or white background

VERIFICATION:
- After adding the image to the repository, verify it displays correctly in the documentation
- Check that the image is properly referenced in src/12_learning_rate.md
- Ensure the image is clear and readable at different zoom levels