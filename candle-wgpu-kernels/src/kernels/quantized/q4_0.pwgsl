#define BLOCK_SIZE 32
#define BIT_SIZE 4

#define D_OFFSET 0
#define QS_OFFSET 2

#define TS 1
#define WGS 1
#define TID_SIZE 4

//tid0 loads 0-3 and 16-19, (loads 8 value), but the stride for tid1 is 4 (it must load) 4-7 and 20-24
#define STRIDE_WIDTHB 4
#define CUSTOM_QUANTIZED_INDEX(i, base) base + i % 4 + (i / 4u) * 16u
// #pp_begin PRE_CUSTOM_QUANTIZED_INDEX_INSTRUCTIONS()
// let is_last_u2 = (lk + 1u) / TID_SIZE;
// #pp_end

// #define CUSTOM_QUANTIZED_INDEX(i, base) ((base / 32) * 32) + (base % 32 + (i % 6) + ((i / 6u) ^ (((i % 6u) / 4u) * is_last_u2)) * 16u) % 32

//0-3 or 2-5 makes 6 values for the first 4, and 16-19 and 18-21 makes the other 6 needed for A
//#define SIZE_AREG 12

#pp_begin LOAD_HEADER(data)
let block_starting_byte = block_index * BLOCK_BYTE_SIZE;
let block_qs_byte = block_starting_byte + QS_OFFSET;
let header_data1 = load_u32(data, block_starting_byte / 4u);
let d = half_to_float((header_data1 >> (((block_starting_byte) % 4) * 8)) & 0xFFFF);
#pp_end

#pp_begin LOAD_SINGLE(data, tid)

    let THREADS_PER_BLOCK = BLOCK_SIZE / VALUES_PER_THREAD;

    // ---- block invariants ----
    let shift_bytes = block_qs_byte & 3u;
    let out_base    = //shift_bytes + 
        tid * 4u;

    let base_word  = block_qs_byte >> 2u;
    let word_index = base_word + tid;

    // ---- load ----
    let w = load_u32(data, word_index);

    var x_u32 = w;

    // only even blocks cross a u32 boundary
    if (shift_bytes > 0) {
        let w_next = load_u32(data, word_index + 1u);
        // take last 2 bytes of w and first 2 bytes of w_next
        x_u32 = (w >> 16u) | (w_next << 16u);
    }

    // ---- decode ----
    #define DEFAULT_M_OFFSET 8
    #definec BLOCK_MASK (BLOCK_SIZE / 2u - 1u)

    var tmp = x_u32;

    // unrolled
    // for (var i = 0u; i < 4u; i++) {
    //     let byte = tmp & 0xFFu;
    //     tmp >>= 8u;

    //     let idx = (out_base + i) & BLOCK_MASK;

    //     let lo = f32(i32(byte & 0x0Fu) - DEFAULT_M_OFFSET) * d;
    //     CALLBACK(idx,      lo, i)

    //     let hi = f32(i32(byte >> 4)    - DEFAULT_M_OFFSET) * d;
    //     CALLBACK(idx + 16, hi, i + 4u)
    // }


    var byte = tmp & 0xFFu;    
    tmp >>= 8u;
    var idx = (out_base + 0u) & BLOCK_MASK;
    var lo = f32(i32(byte & 0x0Fu) - DEFAULT_M_OFFSET) * d;
    CALLBACK(idx,      lo, 0u)
    var hi = f32(i32(byte >> 4)    - DEFAULT_M_OFFSET) * d;
    CALLBACK(idx + 16, hi, 0u + 4u)

    byte = tmp & 0xFFu;    
    tmp >>= 8u;
    idx = (out_base + 1u) & BLOCK_MASK;
    lo = f32(i32(byte & 0x0Fu) - DEFAULT_M_OFFSET) * d;
    CALLBACK(idx,      lo, 1u)
    hi = f32(i32(byte >> 4)    - DEFAULT_M_OFFSET) * d;
    CALLBACK(idx + 16, hi, 1u+ 4u)

    byte = tmp & 0xFFu;    
    tmp >>= 8u;
    idx = (out_base + 2u) & BLOCK_MASK;
    lo = f32(i32(byte & 0x0Fu) - DEFAULT_M_OFFSET) * d;
    CALLBACK(idx,      lo, 2u)
    hi = f32(i32(byte >> 4)    - DEFAULT_M_OFFSET) * d;
    CALLBACK(idx + 16, hi, 2u + 4u)

    byte = tmp & 0xFFu;    
    tmp >>= 8u;
    idx = (out_base + 3u) & BLOCK_MASK;
    lo = f32(i32(byte & 0x0Fu) - DEFAULT_M_OFFSET) * d;
    CALLBACK(idx,      lo, 3u)
    hi = f32(i32(byte >> 4)    - DEFAULT_M_OFFSET) * d;
    CALLBACK(idx + 16, hi, 3u + 4u)





#pp_end


#include "quantHelper.pwgsl"

#ifdef SGEMM

#define WIDTHA 4u
#definec WIDTHB VALUES_PER_THREAD 
#include "matmulHelperSgemm.pwgsl"
MATMUL(matmul_sgemm)
#endif

